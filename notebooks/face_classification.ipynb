{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform face classification using VGGface\n",
    "\n",
    "## 1. First resolving version conflict of the default vggface package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20951"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resolve version conflict from one of the files\n",
    "filename = \"/home/eric/anaconda3/envs/peekingduckling/lib/python3.8/site-packages/keras_vggface/models.py\"\n",
    "text = open(filename).read()\n",
    "open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "from keras_vggface.utils import preprocess_input\n",
    "import numpy as np\n",
    "from numpy import asarray\n",
    "from PIL import Image\n",
    "from tensorflow import keras\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs: [<KerasTensor: shape=(None, 224, 224, 3) dtype=float32 (created by layer 'input_6')>]\n",
      "Outputs: [<KerasTensor: shape=(None, 2048) dtype=float32 (created by layer 'global_average_pooling2d_3')>]\n"
     ]
    }
   ],
   "source": [
    "# Loading VGGFace model without classifier and getting output to be a face embedding\n",
    "base_model = VGGFace(model='resnet50', include_top=False, input_shape=(224, 224, 3), pooling='avg')\n",
    "# summarize input and output shape\n",
    "print('Inputs: %s' % base_model.inputs)\n",
    "print('Outputs: %s' % base_model.outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Obtain image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract faces and calculate face embeddings for a list of photo files\n",
    "def get_embeddings(files, model):\n",
    "\t# extract faces\n",
    "\tfaces = [f for f in files]\n",
    "\t# convert into an array of samples\n",
    "\tsamples = asarray(faces, 'float32')\n",
    "\t# prepare the face for the model, e.g. center pixels\n",
    "\tsamples = preprocess_input(samples, version=2)\n",
    "\t# create a vggface model\n",
    "\t# perform prediction\n",
    "\tyhat = model.predict(samples)\n",
    "\treturn yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Resizing(224,224),\n",
    "    # keras.layers.RandomZoom((-0.2, 0.2), fill_mode='reflect'),\n",
    "    # keras.layers.RandomTranslation(0.2, 0.2, fill_mode='reflect'),\n",
    "    # keras.layers.RandomRotation(0.2, fill_mode='reflect'),\n",
    "    # keras.layers.RandomFlip(),\n",
    "    # keras.layers.RandomContrast(0.2),\n",
    "    base_model,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare image embeddings between two different person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../data/raw/images/clarence/300_clarence.jpg'\n",
    "img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(img)\n",
    "clarence_embedding = get_embeddings([img], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7209997 , 0.        , 0.03487726, ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clarence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '../data/raw/images/eric_kwok/0_eric_kwok.jpg'\n",
    "img = Image.open(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array(img)\n",
    "kwok_embedding = get_embeddings([img], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.646888 , 0.0573287, 0.       , ..., 5.8668456, 0.       ,\n",
       "        0.       ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwok_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101.5161"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison of embedding using euclidean distance\n",
    "diff_dist = np.linalg.norm(clarence_embedding-kwok_embedding)\n",
    "diff_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.576074093580246"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparring using cosine difference (1 - cosine distance)\n",
    "cos_sim = cosine(clarence_embedding, kwok_embedding)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare embeddings for the same person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.50315"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = '../data/raw/images/clarence/300_clarence.jpg'\n",
    "img = Image.open(image_path)\n",
    "\n",
    "img = np.array(img)\n",
    "clarence_embedding_2 = get_embeddings([img], model)\n",
    "\n",
    "# Comparison of embedding\n",
    "diff_dist = np.linalg.norm(clarence_embedding-clarence_embedding_2)\n",
    "diff_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46248912811279297"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparring using cosine distance (1 - cosine similarity)\n",
    "cos_sim = cosine(clarence_embedding, clarence_embedding_2)\n",
    "cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Get the embeddings from clarence, eric kwok and eric lee"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5b274a045f9f8cfd523ed59dec27e79480dd61f3c666e1623895b763e24bf196"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('peekingduckling': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
